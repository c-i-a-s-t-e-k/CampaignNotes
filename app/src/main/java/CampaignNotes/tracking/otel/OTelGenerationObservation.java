package CampaignNotes.tracking.otel;

import java.time.Instant;
import java.util.Map;

import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.SpanKind;
import io.opentelemetry.api.trace.StatusCode;
import io.opentelemetry.api.trace.Tracer;
import io.opentelemetry.context.Context;

/**
 * Observation for tracking LLM text generation operations.
 * 
 * This class creates a child span under a parent trace to track
 * LLM calls for tasks like:
 * - Note Artifact Extraction (NAE)
 * - Artifact Relationship Extraction (ARE)
 * - Any other LLM-based text generation
 * 
 * It captures:
 * - Model used
 * - Input prompt
 * - Generated response
 * - Token usage (input, output, total)
 * - Component and stage identification
 * - Success/failure status
 * 
 * Usage within a parent trace:
 * <pre>
 * try (OTelTrace trace = traceManager.createTrace(...)) {
 *     try (OTelGenerationObservation obs = 
 *         new OTelGenerationObservation("nae-generation", trace.getContext())) {
 *         
 *         obs.withModel("gpt-4")
 *            .withPrompt(prompt)
 *            .withComponent("nae")
 *            .withStage("artifact-extraction");
 *         
 *         // Call LLM...
 *         String response = llm.generate(prompt);
 *         
 *         obs.withResponse(response)
 *            .withTokenUsage(inputTokens, outputTokens, totalTokens)
 *            .setSuccess();
 *     }
 * }
 * </pre>
 */
public class OTelGenerationObservation implements AutoCloseable {
    
    private final Span span;
    
    /**
     * Creates a new generation observation as a child of the parent context.
     * 
     * @param name observation name (e.g., "nae-generation", "are-generation")
     * @param parentContext context from the parent trace
     */
    public OTelGenerationObservation(String name, Context parentContext) {
        Tracer tracer = OpenTelemetryConfig.getTracer();
        this.span = tracer.spanBuilder(name)
            .setParent(parentContext)
            .setSpanKind(SpanKind.CLIENT)  // LLM call is an external service
            .startSpan();
        
        // Explicitly set observation type for Langfuse
        span.setAttribute("langfuse.observation.type", "generation");
    }
    
    /**
     * Sets the model used for generation.
     * Uses OpenTelemetry semantic conventions for generative AI.
     * 
     * @param model model identifier (e.g., "gpt-4", "o3-mini")
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withModel(String model) {
        span.setAttribute("gen_ai.system", "openai");
        span.setAttribute("gen_ai.request.model", model);
        return this;
    }
    
    /**
     * Sets the input prompt sent to the LLM.
     * Truncated to 1000 characters to avoid excessive span size.
     * 
     * @param prompt the prompt text
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withPrompt(String prompt) {
        span.setAttribute("gen_ai.prompt", prompt);
        return this;
    }
    
    /**
     * Sets the response generated by the LLM.
     * Truncated to 1000 characters to avoid excessive span size.
     * 
     * @param response the generated text
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withResponse(String response) {
        span.setAttribute("gen_ai.completion", response);
        return this;
    }
    
    /**
     * Sets the token usage for the LLM call.
     * 
     * @param inputTokens tokens in the prompt
     * @param outputTokens tokens in the response
     * @param totalTokens total tokens used
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withTokenUsage(int inputTokens, int outputTokens, int totalTokens) {
        span.setAttribute("gen_ai.usage.input_tokens", inputTokens);
        span.setAttribute("gen_ai.usage.output_tokens", outputTokens);
        span.setAttribute("gen_ai.usage.total_tokens", totalTokens);
        return this;
    }
    
    /**
     * Sets the component identifier for this generation.
     * 
     * @param component component name (e.g., "nae", "are")
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withComponent(String component) {
        span.setAttribute("component", component);
        return this;
    }
    
    /**
     * Sets the processing stage identifier.
     * 
     * @param stage stage name (e.g., "artifact-extraction", "relationship-extraction")
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withStage(String stage) {
        span.setAttribute("stage", stage);
        return this;
    }
    
    /**
     * Sets the actual model returned by the LLM API.
     * This may differ from the requested model (e.g., when using aliases).
     * 
     * @param model the response model identifier
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withResponseModel(String model) {
        span.setAttribute("gen_ai.response.model", model);
        return this;
    }
    
    /**
     * Sets the cost of the LLM call in USD.
     * 
     * @param cost the cost in dollars
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withCost(double cost) {
        span.setAttribute("gen_ai.usage.cost", cost);
        return this;
    }
    
    /**
     * Sets model parameters used for the generation.
     * 
     * @param parameters map of parameter names to values (e.g., temperature, max_tokens)
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withModelParameters(Map<String, Object> parameters) {
        for (Map.Entry<String, Object> entry : parameters.entrySet()) {
            String key = "gen_ai.request." + entry.getKey();
            Object value = entry.getValue();
            
            if (value instanceof String s) {
                span.setAttribute(key, s);
            } else if (value instanceof Long l) {
                span.setAttribute(key, l);
            } else if (value instanceof Integer i) {
                span.setAttribute(key, i.longValue());
            } else if (value instanceof Double d) {
                span.setAttribute(key, d);
            } else if (value instanceof Boolean b) {
                span.setAttribute(key, b);
            } else if (value != null) {
                span.setAttribute(key, value.toString());
            }
        }
        return this;
    }
    
    /**
     * Sets the timestamp when the LLM started generating the completion.
     * 
     * @param startTime the completion start time
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withCompletionStartTime(Instant startTime) {
        span.setAttribute("langfuse.observation.completion_start_time", startTime.toString());
        return this;
    }
    
    /**
     * Links this generation to a versioned prompt in Langfuse.
     * 
     * @param promptName the name of the prompt in Langfuse
     * @param promptVersion the version number of the prompt
     * @return this observation for method chaining
     */
    public OTelGenerationObservation withPrompt(String promptName, int promptVersion) {
        span.setAttribute("langfuse.observation.prompt.name", promptName);
        span.setAttribute("langfuse.observation.prompt.version", promptVersion);
        return this;
    }
    
    /**
     * Marks the observation as successful.
     */
    public void setSuccess() {
        span.setStatus(StatusCode.OK);
    }
    
    /**
     * Marks the observation as failed with an error message.
     * 
     * @param message error description
     */
    public void setError(String message) {
        span.setStatus(StatusCode.ERROR, message);
    }
    
    /**
     * Records an exception and marks the observation as failed.
     * 
     * @param e the exception that occurred
     */
    public void recordException(Exception e) {
        span.recordException(e);
        span.setStatus(StatusCode.ERROR, e.getMessage());
    }
    
    /**
     * Ends the span and exports it.
     * Automatically called when used with try-with-resources.
     */
    @Override
    public void close() {
        span.end();
    }
}
